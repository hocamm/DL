{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import multilingual_t5.preprocessors\n",
    "import multilingual_t5.tasks\n",
    "import multilingual_t5.utils\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import unicodedata\n",
    "from t5.evaluation import qa_utils\n",
    "from absl.testing import absltest\n",
    "from multilingual_t5.evaluation import metrics\n",
    "from t5.evaluation import test_utils\n",
    "from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Union      \n",
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from flax.core import scope as flax_scope\n",
    "from flax.training import common_utils\n",
    "import jax\n",
    "from jax import lax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import seqio\n",
    "from t5x import losses as t5x_losses\n",
    "from t5x import metrics as metrics_lib\n",
    "from t5x import models as t5x_models\n",
    "from t5x import utils as t5x_utils\n",
    "from t5x_retrieval import utils\n",
    "import tensorflow as tf\n",
    "from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Union\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "from flask import request\n",
    "import requests\n",
    "from googletrans import Translator\n",
    "import string\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "def _string_join(lst): \n",
    "  out = tf.strings.join(lst, separator=' ')\n",
    "  return tf.strings.regex_replace(out, r'\\s+', ' ')\n",
    "def _pad_punctuation(text):\n",
    "  text = tf.strings.regex_replace(text, r'([^_\\s\\p{N}\\p{L}\\p{M}])', r' \\1 ')\n",
    "  text = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "  return text\n",
    "\n",
    "def xnli_map_hypothesis_premise(dataset, target_language): #XNLI dataset preparation\n",
    "  def _process(x):\n",
    "    languages = x['hypothesis']['language']\n",
    "    translations = x['hypothesis']['translation']\n",
    "    label = tf.fill(tf.shape(languages), x['label'])\n",
    "    premise = tf.fill(tf.shape(languages), x['premise'][target_language])\n",
    "\n",
    "    return {\n",
    "        'language': languages,\n",
    "        'translation': translations,\n",
    "        'label': label,\n",
    "        'premise': premise\n",
    "    }\n",
    "\n",
    "  dataset = dataset.map(\n",
    "      _process, num_parallel_calls=tf.data.experimental.AUTOTUNE).unbatch()\n",
    "  dataset = dataset.filter(\n",
    "      lambda x: tf.math.equal(x['language'], target_language))\n",
    "  return dataset\n",
    "\n",
    "def wikiann(dataset):\n",
    "  def _process(x, delimiter=' $$ '):\n",
    "    inputs = 'tag: ' + tf.strings.reduce_join(x['tokens'], separator=' ')\n",
    "    targets = tf.strings.reduce_join(x['spans'], separator=delimiter)\n",
    "\n",
    "    return {\n",
    "        'inputs': inputs,\n",
    "        'targets': targets,\n",
    "        'tokens': x['tokens'],\n",
    "        'tags': x['tags'],\n",
    "        'langs': x['langs'],\n",
    "        'spans': x['spans']\n",
    "    }\n",
    "  return dataset.map(_process, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "def process_mnli(dataset):\n",
    "\n",
    "  def _process(x):\n",
    "    return {\n",
    "        'inputs': tf.strings.join(['xnli: premise: ', x['premise'],\n",
    "                                   ' hypothesis: ', x['hypothesis']]),\n",
    "        'targets': tf.strings.as_string(x['label'])\n",
    "    }\n",
    "\n",
    "  return dataset.map(_process, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "def process_xnli(dataset, target_languages):\n",
    "  def _process(x):\n",
    "    return {\n",
    "        'inputs': tf.strings.join(['xnli: premise: ', x['premise'],\n",
    "                                   ' hypothesis: ', x['translation']]),\n",
    "        'targets': tf.strings.as_string(x['label'])\n",
    "    }\n",
    "\n",
    "  output = []\n",
    "  for language in target_languages:\n",
    "    examples = xnli_map_hypothesis_premise(dataset, target_language=language)\n",
    "    d = examples.map(_process, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    output.append(d)\n",
    "\n",
    "  output_dataset = output[0]\n",
    "  for lang_dataset in output[1:]:\n",
    "    output_dataset = output_dataset.concatenate(lang_dataset)\n",
    "  return output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_mlqa(s, lang, punct):\n",
    "\n",
    "\n",
    "  whitespace_langs = ['en', 'es', 'hi', 'vi', 'de', 'ar']\n",
    "  mixed_segmentation_langs = ['zh']\n",
    "\n",
    "  def whitespace_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "  def mixed_segmentation(text):\n",
    "    segs_out = []\n",
    "    temp_str = ''\n",
    "    for char in text:\n",
    "      if re.search(r'[\\u4e00-\\u9fa5]', char) or char in punct:\n",
    "        if temp_str != '':\n",
    "          ss = whitespace_tokenize(temp_str)\n",
    "          segs_out.extend(ss)\n",
    "          temp_str = ''\n",
    "        segs_out.append(char)\n",
    "      else:\n",
    "        temp_str += char\n",
    "    if temp_str != '':\n",
    "      ss = whitespace_tokenize(temp_str)\n",
    "      segs_out.extend(ss)\n",
    "    return segs_out\n",
    "\n",
    "  def drop_articles(text, lang):\n",
    "    if lang == 'en':\n",
    "      return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    elif lang == 'es':\n",
    "      return re.sub(r'\\b(un|una|unos|unas|el|la|los|las)\\b', ' ', text)\n",
    "    elif lang == 'hi':\n",
    "      return text\n",
    "    elif lang == 'vi':\n",
    "      return re.sub(r'\\b(của|là|cái|chiếc|những)\\b', ' ', text)\n",
    "    elif lang == 'de':\n",
    "      return re.sub(\n",
    "          r'\\b(ein|eine|einen|einem|eines|einer|der|die|das|den|dem|des)\\b',\n",
    "          ' ', text)\n",
    "    elif lang == 'ar':\n",
    "      return re.sub('\\sال^|ال', ' ', text)\n",
    "    elif lang == 'zh':\n",
    "      return text\n",
    "\n",
    "  def white_space_fix(text, lang):\n",
    "    if lang in whitespace_langs:\n",
    "      tokens = whitespace_tokenize(text)\n",
    "    elif lang in mixed_segmentation_langs:\n",
    "      tokens = mixed_segmentation(text)\n",
    "    return ' '.join([t for t in tokens if t.strip()])\n",
    "\n",
    "  def drop_punc(text):\n",
    "    return ''.join(c for c in text if c not in punct)\n",
    "\n",
    "  s = s.lower()\n",
    "  s = drop_punc(s)\n",
    "  s = drop_articles(s, lang)\n",
    "  s = white_space_fix(s, lang)\n",
    "  return s\n",
    "\n",
    "\n",
    "def mlqa(targets, predictions, lang=None):\n",
    "\n",
    "  assert lang is not None\n",
    "  punct = {\n",
    "      chr(i)\n",
    "      for i in range(sys.maxunicode)\n",
    "      if unicodedata.category(chr(i)).startswith('P')\n",
    "  }.union(string.punctuation)\n",
    "  targets = [[normalize_mlqa(t, lang, punct) for t in u] for u in targets]\n",
    "  predictions = [normalize_mlqa(p, lang, punct) for p in predictions]\n",
    "  return qa_utils.qa_metrics(targets, predictions)\n",
    "\n",
    "\n",
    "def span_f1(targets, predictions):\n",
    "\n",
    "  true_positives = collections.defaultdict(int)\n",
    "  false_positives = collections.defaultdict(int)\n",
    "  false_negatives = collections.defaultdict(int)\n",
    "\n",
    "  def tags_to_spans(tag_sequence, delimiter=' $$ '):\n",
    "    tag_sequence_split = [x.strip() for x in tag_sequence.split(delimiter)]\n",
    "    tags_entities = []\n",
    "    for tag_entity in tag_sequence_split:\n",
    "      tag_entity_split = tag_entity.split(':')\n",
    "      if len(tag_entity_split) != 2:\n",
    "        continue\n",
    "      tag = tag_entity_split[0].strip()\n",
    "      entity = tag_entity_split[1].strip()\n",
    "      tags_entities.append((tag, entity))\n",
    "    return tags_entities\n",
    "\n",
    "  def compute_f1_metrics(true_positives, false_positives, false_negatives):\n",
    "    precision = float(true_positives) / float(true_positives + false_positives +\n",
    "                                              1e-13)\n",
    "    recall = float(true_positives) / float(true_positives + false_negatives +\n",
    "                                           1e-13)\n",
    "    f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n",
    "    return precision, recall, f1_measure\n",
    "\n",
    "  for target, pred in zip(targets, predictions):\n",
    "    gold_spans = tags_to_spans(target)\n",
    "    predicted_spans = tags_to_spans(pred)\n",
    "\n",
    "    for span in predicted_spans:\n",
    "      if span in gold_spans:\n",
    "        true_positives[span[0]] += 1\n",
    "        gold_spans.remove(span)\n",
    "      else:\n",
    "        false_positives[span[0]] += 1\n",
    "    # These spans weren't predicted.\n",
    "    for span in gold_spans:\n",
    "      false_negatives[span[0]] += 1\n",
    "\n",
    "  _, _, f1_measure = compute_f1_metrics(\n",
    "      sum(true_positives.values()), sum(false_positives.values()),\n",
    "      sum(false_negatives.values()))\n",
    "\n",
    "  return {'span_f1': f1_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetricsTest(test_utils.BaseMetricsTest):\n",
    "\n",
    "  def test_same_mlqa(self):\n",
    "    ref = \"this is a string\"\n",
    "    self.assertDictClose(\n",
    "        metrics.mlqa([[\"\", ref], [ref, ref]], [ref, ref], lang=\"en\"), {\n",
    "            \"em\": 100,\n",
    "            \"f1\": 100,\n",
    "        })\n",
    "\n",
    "  def test_different_mlqa(self):\n",
    "    ref = \"this is a string\"\n",
    "    self.assertDictClose(\n",
    "        metrics.mlqa([[ref, ref], [ref, ref]], [\"\", \"\"], lang=\"en\"), {\n",
    "            \"em\": 0,\n",
    "            \"f1\": 0\n",
    "        })\n",
    "\n",
    "  def test_article_drop_mlqa(self):\n",
    "    ref = \"this unas a string\"\n",
    "    pred = \"this a string\"\n",
    "    self.assertDictClose(\n",
    "        metrics.mlqa([[ref]], [pred], lang=\"es\"), {\n",
    "            \"em\": 100,\n",
    "            \"f1\": 100,\n",
    "        })\n",
    "\n",
    "  def test_mlqa_small(self):\n",
    "    self.assertDictClose(\n",
    "        metrics.mlqa([[\"abc abd\", \"$$$$\"]], [\"abd\"], lang=\"en\"),\n",
    "        {\"f1\": 100 * 2.0 / 3.0, \"em\": 0.},\n",
    "    )\n",
    "if __name__ == \"__main__\":\n",
    "  absltest.main()\n",
    "\n",
    "  \"\"\"Add Tasks to registry.\"\"\"\n",
    "import functools\n",
    "\n",
    "import seqio\n",
    "import t5.data\n",
    "\n",
    "tsv_english_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_english.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_english.tsv\",\n",
    "}\n",
    "\n",
    "tsv_german_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_german.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_german.tsv\",\n",
    "}\n",
    "\n",
    "tsv_arabic_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_arabic.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_arabic.tsv\",\n",
    "}\n",
    "\n",
    "tsv_chinese_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_chinese.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_chinese.tsv\",\n",
    "}\n",
    "\n",
    "tsv_dutch_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_dutch.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_dutch.tsv\",\n",
    "}\n",
    "\n",
    "tsv_french_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_french.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_french.tsv\",\n",
    "}\n",
    "\n",
    "tsv_hindi_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_hindi.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_hindi.tsv\",\n",
    "}\n",
    "\n",
    "tsv_indonesian_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_indonesian.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_indonesian.tsv\",\n",
    "}\n",
    "\n",
    "tsv_japanese_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_japanese.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_japanese.tsv\",\n",
    "}\n",
    "\n",
    "tsv_portuguese_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_portuguese.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_portuguese.tsv\",\n",
    "}\n",
    "\n",
    "tsv_russian_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_russian.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_russian.tsv\",\n",
    "}\n",
    "\n",
    "language_to_path = {\n",
    "    \"arabic\": tsv_arabic_path,\n",
    "    \"english\": tsv_english_path,\n",
    "    \"russian\": tsv_russian_path,\n",
    "    \"portuguese\": tsv_portuguese_path,\n",
    "    \"japanese\": tsv_japanese_path,\n",
    "    \"hindi\": tsv_hindi_path,\n",
    "    \"indonesian\": tsv_indonesian_path,\n",
    "    \"dutch\": tsv_dutch_path,\n",
    "    \"german\": tsv_german_path,\n",
    "    \"chinese\": tsv_chinese_path,\n",
    "    \"french\": tsv_french_path,\n",
    "\n",
    "}\n",
    "\n",
    "tsv_english_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_english_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_english.tsv\",\n",
    "}\n",
    "\n",
    "tsv_german_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_german_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_german.tsv\",\n",
    "}\n",
    "\n",
    "tsv_arabic_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_arabic_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_arabic.tsv\",\n",
    "}\n",
    "\n",
    "tsv_chinese_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_chinese_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_chinese.tsv\",\n",
    "}\n",
    "\n",
    "tsv_dutch_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_dutch_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_dutch.tsv\",\n",
    "}\n",
    "\n",
    "tsv_french_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_french_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_french.tsv\",\n",
    "}\n",
    "\n",
    "tsv_hindi_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_hindi_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_hindi.tsv\",\n",
    "}\n",
    "\n",
    "tsv_indonesian_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_indonesian_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_indonesian.tsv\",\n",
    "}\n",
    "\n",
    "tsv_japanese_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_japanese_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_japanese.tsv\",\n",
    "}\n",
    "\n",
    "tsv_portuguese_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_portuguese_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_portuguese.tsv\",\n",
    "}\n",
    "\n",
    "tsv_russian_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_russian_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_russian.tsv\",\n",
    "}\n",
    "\n",
    "tsv_spanish_negatives_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_spanish_negatives.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_spanish.tsv\",\n",
    "}\n",
    "\n",
    "language_negatives_to_path = {\n",
    "    \"arabic\": tsv_arabic_negatives_path,\n",
    "    \"english\": tsv_english_negatives_path,\n",
    "    \"russian\": tsv_russian_negatives_path,\n",
    "    \"portuguese\": tsv_portuguese_negatives_path,\n",
    "    \"japanese\": tsv_japanese_negatives_path,\n",
    "    \"hindi\": tsv_hindi_negatives_path,\n",
    "    \"indonesian\": tsv_indonesian_negatives_path,\n",
    "    \"dutch\": tsv_dutch_negatives_path,\n",
    "    \"german\": tsv_german_negatives_path,\n",
    "    \"chinese\": tsv_chinese_negatives_path,\n",
    "    \"french\": tsv_french_negatives_path,\n",
    "    \"spanish\": tsv_spanish_negatives_path,\n",
    "}\n",
    "\n",
    "tsv_clirmatrix_multi_path = {\n",
    "        \"train\": \"/home/mac/datasets/train_clirmatrix.tsv\",\n",
    "        \"validation\": \"/home/mac/datasets/dev_clirmatrix.tsv\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "MULTILIGUAL_SPM_PATH = \"gs://t5-data/vocabs/mc4.250000.100extra/sentencepiece.model\"  # GCS\n",
    "MULTILIGUAL_EXTRA_IDS = 100\n",
    "\n",
    "\n",
    "def get_multilingual_vocabulary():\n",
    "  return seqio.SentencePieceVocabulary(MULTILIGUAL_SPM_PATH)\n",
    "\n",
    "\n",
    "DEFAULT_VOCAB = t5.data.get_default_vocabulary()\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\":\n",
    "        seqio.Feature(vocabulary=DEFAULT_VOCAB, add_eos=True, required=False),\n",
    "    \"targets\":\n",
    "        seqio.Feature(vocabulary=DEFAULT_VOCAB, add_eos=True)\n",
    "}\n",
    "\n",
    "\n",
    "MULTILINGUAL_VOCAB = get_multilingual_vocabulary()\n",
    "MULTILINGUAL_OUTPUT_FEATURES = {\n",
    "    \"inputs\":\n",
    "        seqio.Feature(vocabulary=MULTILINGUAL_VOCAB, add_eos=True, required=False),\n",
    "    \"targets\":\n",
    "        seqio.Feature(vocabulary=MULTILINGUAL_VOCAB, add_eos=True)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "MULTILINGUAL_OUTPUT_FEATURES_NEGATIVES = {\n",
    "    \"inputs\":\n",
    "        seqio.Feature(vocabulary=MULTILINGUAL_VOCAB, add_eos=True, required=False),\n",
    "    \"targets\":\n",
    "        seqio.Feature(vocabulary=MULTILINGUAL_VOCAB, add_eos=True),\n",
    "    \"negative_targets\":\n",
    "        seqio.Feature(vocabulary=MULTILINGUAL_VOCAB, add_eos=True),\n",
    "}\n",
    "\n",
    "seqio.TaskRegistry.add(\n",
    "    \"beir_msmarco_retrieval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"beir/msmarco:1.0.0\",\n",
    "        splits={\n",
    "            \"train\": \"train\",\n",
    "            \"validation\": \"validation\",\n",
    "        },\n",
    "    ),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            t5.data.preprocessors.rekey,\n",
    "            key_map={\n",
    "                \"inputs\": \"query\",\n",
    "                \"targets\": \"passage\",\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "for language in list(language_to_path.keys()):\n",
    "    seqio.TaskRegistry.add(\n",
    "        f\"mmarco_retrieval_{language}\",\n",
    "        source=seqio.TextLineDataSource(\n",
    "            split_to_filepattern=language_to_path[language],\n",
    "            ),\n",
    "        preprocessors=[\n",
    "        functools.partial(\n",
    "            t5.data.preprocessors.parse_tsv,\n",
    "            field_names=[\"inputs\",\"targets\"]),\n",
    "                seqio.preprocessors.tokenize,\n",
    "                seqio.CacheDatasetPlaceholder(),\n",
    "                seqio.preprocessors.append_eos_after_trim,\n",
    "                ],\n",
    "        metric_fns=[],\n",
    "        output_features=MULTILINGUAL_OUTPUT_FEATURES,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "seqio.MixtureRegistry.add(\n",
    "  \"multilingual_marco_mixture\",\n",
    "  [(f\"mmarco_retrieval_{language}\", 1) for language in list(language_to_path.keys())]\n",
    ")\n",
    "\n",
    "for language in list(language_to_path.keys()):\n",
    "    seqio.TaskRegistry.add(\n",
    "        f\"mmarco_retrieval_{language}_negatives\",\n",
    "        source=seqio.TextLineDataSource(\n",
    "            split_to_filepattern=language_negatives_to_path[language],\n",
    "            ),\n",
    "        preprocessors=[\n",
    "        functools.partial(\n",
    "            t5.data.preprocessors.parse_tsv,\n",
    "            field_names=[\"inputs\",\"targets\", \"negative_targets\"]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[],\n",
    "        output_features=MULTILINGUAL_OUTPUT_FEATURES_NEGATIVES,\n",
    "    ) \n",
    "\n",
    "seqio.MixtureRegistry.add(\n",
    "  \"multilingual_marco_mixture_negatives\",\n",
    "  [(f\"mmarco_retrieval_{language}_negatives\", 1) for language in list(language_to_path.keys())]\n",
    ")\n",
    "seqio.TaskRegistry.add(\n",
    "        f\"clirmatrix_pretraining\",\n",
    "        source=seqio.TextLineDataSource(\n",
    "            split_to_filepattern=tsv_clirmatrix_multi_path,\n",
    "            ),\n",
    "        preprocessors=[\n",
    "        functools.partial(\n",
    "            t5.data.preprocessors.parse_tsv,\n",
    "            field_names=[\"inputs\",\"targets\"]),\n",
    "                seqio.preprocessors.tokenize,\n",
    "                seqio.CacheDatasetPlaceholder(),\n",
    "                seqio.preprocessors.append_eos_after_trim,\n",
    "                ],\n",
    "        metric_fns=[],\n",
    "        output_features=MULTILINGUAL_OUTPUT_FEATURES,\n",
    "    )\n",
    "\n",
    "for split in [\"query\", \"passage\"]:\n",
    "  seqio.TaskRegistry.add(\n",
    "      f\"beir_msmarco_retrieval_{split}\",\n",
    "      source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"beir/msmarco:1.0.0\",\n",
    "          splits={split: split},\n",
    "      ),\n",
    "      preprocessors=[\n",
    "          functools.partial(\n",
    "              t5.data.preprocessors.rekey,\n",
    "              key_map={\n",
    "                  \"inputs\": split,\n",
    "                  \"targets\": f\"{split}_id\",\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "      ],\n",
    "      metric_fns=[],\n",
    "      output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def string_return():\n",
    "    resp = \"ML 서버 열심히 만들자\"\n",
    "   \n",
    "\n",
    "\n",
    "    return resp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/sick')\n",
    "def breadpage():\n",
    "   return '식빵맨'\n",
    "\n",
    "\n",
    "@app.route(\"/curry\")\n",
    "def curry():\n",
    "    return '''<!DOCTYPE HTML><html>\n",
    "  <head>\n",
    "    <title>카레빵맨 이미지 받아랑!!!</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Curry bread man</h1>\n",
    "    <img src = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRrfTgmkoNaKpFBUZ_DmdzbkGc66dsch5CWYg&usqp=CAU\"\n",
    "  </body>\n",
    "</html>'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/bread', methods=['GET','POST'])\n",
    "def test_get():\n",
    "    bread_receive = request.args.get('breadname')\n",
    "    return f'이것의 이름은 {bread_receive}'\n",
    "\n",
    "\n",
    "'''\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "\n",
    "@app.route('/turk', methods=['GET','POST'])\n",
    "\n",
    "\n",
    "def test_conv():\n",
    "    talk = request.args.get(\"humantalk\")\n",
    "    #prompt = f'{message}'\n",
    "    prompt = \"bana türkçe cevap ver\" + talk\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.5,\n",
    "        max_tokens=35,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=5,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return f\"ai's response is : {response.choices[0].text.strip()}\"\n",
    "'''\n",
    "\n",
    "\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "@app.route('/turk', methods=['GET','POST'])\n",
    "def test_conv():\n",
    "    if request.method == 'POST':\n",
    "        param = request.get_json()\n",
    "        param = param['content']\n",
    "\n",
    "\n",
    "        pipe = pipeline('text-generation',model = \"redrussianarmy/gpt2-turkish-cased\",\n",
    "                        tokenizer=\"redrussianarmy/gpt2-turkish-cased\")\n",
    "        text = pipe(str(param)+\"gramer doğruluğunu kontrol et:\")[0]['generated_text']\n",
    "\n",
    "\n",
    "        text2 = pipe(\"Aşağıdaki cümle dilbilgisi açısından yanlışsa, lütfen x ile işaretleyin:\"+str(param))[0]['generated_text']\n",
    "        Aanswer = text\n",
    "        translator = Translator()\n",
    "        Ttransans = (translator.translate(Aanswer ,src = \"tr\",dest='ko').text)\n",
    "        ttt = (translator.translate(text2 ,src = \"tr\",dest='ko').text)\n",
    "        AAanswer = jsonify({ 'Aanswer' : Aanswer, \"Ttrans_answer\": Ttransans,\"Iisrightvalue\":text2, \"transIS\":ttt})\n",
    "    return AAanswer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/echo_call/<param>') #get echo api\n",
    "def get_echo_call(param):\n",
    "    return jsonify({\"param\": param})\n",
    "\n",
    "\n",
    "@app.route('/answer',methods=['GET', 'POST']) #post echo api\n",
    "def post_echo_call():\n",
    "    if request.method == 'POST':\n",
    "        param = request.get_json()\n",
    "        param = param['content']\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Sadece türkçe bilen bir arkadaşsın, elinden geldiği kadar doğal sohbet etmeye çalış.\"},\n",
    "            {\"role\": \"user\", \"content\": param},\n",
    "            ]\n",
    "        )\n",
    "        translator = Translator()\n",
    "        answer = completion.choices[0].message.content\n",
    "        transans = (translator.translate(answer ,src = \"tr\",dest='ko').text)\n",
    "        #answer_json1 = jsonify({ 'answer' : answer})\n",
    "        answer = jsonify({ 'answer' : answer, \"trans_answer\": transans})\n",
    "        return answer\n",
    "\n",
    "\n",
    "    elif request.method =='GET':\n",
    "        return \"get\"\n",
    "    else:\n",
    "        return \"hi\"\n",
    "\n",
    "\n",
    "@app.route('/grammar',methods=['GET', 'POST']) #post echo api\n",
    "def post_grm():\n",
    "    if request.method == 'POST':\n",
    "        param = request.get_json()\n",
    "        param = param['content']\n",
    "\n",
    "\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature = 0.2,\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Lütfen yalnızca dilbilgisi düzeltilmiş cümleleri yazdırın\"},\n",
    "        {\"role\": \"user\", \"content\": param},\n",
    "        #{\"role\":\"system\",\"content\": \"\"},\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "        answer0 = completion.choices[0].message.content\n",
    "        answer = \"doğru cevap : \" + answer0\n",
    "\n",
    "\n",
    "        completion2 = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature = 0,\n",
    "        max_tokens = 2000,\n",
    "        top_p = 1.0,\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Düzeltmek için bir araç olmasını istiyorum. Size metni vereceğim ve yazım, dilbilgisi veya noktalama hataları için incelemenizi isteyeceğim. Metni inceledikten sonra cümlelerden herhangi biri yanlışsa neden yanlış olduğunu açıklayınız.\"},\n",
    "        {\"role\": \"user\", \"content\": param},\n",
    "        #{\"role\":\"system\",\"content\": \"\"},hu\n",
    "\n",
    "\n",
    "        ## Lütfen gramerin neden yanlış olduğunu söyle\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "        reason = completion2.choices[0].message.content\n",
    "\n",
    "for split in [\"query\", \"passage\"]:\n",
    "  seqio.TaskRegistry.add(\n",
    "      f\"mmarco_retrieval_de_{split}\",\n",
    "      source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"mrtydi/mmarco-en:1.0.0\",\n",
    "          splits={split: split},\n",
    "      ),\n",
    "      preprocessors=[\n",
    "          functools.partial(\n",
    "              t5.data.preprocessors.rekey,\n",
    "              key_map={\n",
    "                  \"inputs\": split,\n",
    "                  \"targets\": f\"{split}_id\",\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "      ],\n",
    "      metric_fns=[],\n",
    "      output_features=DEFAULT_OUTPUT_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array = Union[np.ndarray, jnp.ndarray, jax.pxla.ShardedDeviceArray, tf.Tensor]\n",
    "DType = jnp.dtype\n",
    "ConfigDict = ml_collections.ConfigDict\n",
    "PyTreeDef = type(jax.tree_structure(None))\n",
    "Optimizer = optim.Optimizer\n",
    "\n",
    "\n",
    "class DualEncoderBase(t5x_models.BaseTransformerModel):\n",
    "\n",
    "  FEATURE_CONVERTER_CLS: Callable[..., seqio.FeatureConverter]\n",
    "\n",
    "  ALLOWED_INFERENCE_MODE = frozenset({'encode', 'similarity'})\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      module: nn.Module,\n",
    "      feature_converter_cls: Callable[[bool], seqio.FeatureConverter],\n",
    "      input_vocabulary: seqio.Vocabulary,\n",
    "      output_vocabulary: seqio.Vocabulary,\n",
    "      optimizer_def: optim.OptimizerDef,\n",
    "      inference_mode: str = 'encode',\n",
    "  ):\n",
    "    self.FEATURE_CONVERTER_CLS = feature_converter_cls  # pylint: disable=invalid-name\n",
    "    self._inference_mode = inference_mode\n",
    "    super(DualEncoderBase, self).__init__(\n",
    "        module=module,\n",
    "        input_vocabulary=input_vocabulary,\n",
    "        output_vocabulary=output_vocabulary,\n",
    "        optimizer_def=optimizer_def)\n",
    "\n",
    "  def get_initial_variables(\n",
    "      self,\n",
    "      rng: jnp.ndarray,\n",
    "      input_shapes: Mapping[str, Array],\n",
    "      input_types: Optional[Mapping[str, DType]] = None\n",
    "  ) -> flax_scope.FrozenVariableDict:\n",
    "    \"\"\"Get the initial variables for an dual-encoder model.\"\"\"\n",
    "    input_types = {} if input_types is None else input_types\n",
    "    encoder_type = input_types.get('left_encoder_input_tokens', jnp.float32)\n",
    "    left_encoder_shape = input_shapes['left_encoder_input_tokens']\n",
    "    right_encoder_shape = input_shapes['right_encoder_input_tokens']\n",
    "    initial_variables = self.module.init(\n",
    "        rng,\n",
    "        jnp.ones(left_encoder_shape, encoder_type),\n",
    "        jnp.ones(right_encoder_shape, encoder_type),\n",
    "        enable_dropout=False)\n",
    "    return initial_variables\n",
    "\n",
    "  def loss_weights(self, batch: Mapping[str,\n",
    "                                        jnp.ndarray]) -> Optional[jnp.ndarray]:\n",
    "    raise NotImplementedError('Not implemented for dual encoder.')\n",
    "\n",
    "  def predict_batch_with_aux(\n",
    "      self,\n",
    "      params: Mapping[str, Array],\n",
    "      batch: Mapping[str, jnp.ndarray],\n",
    "      rng: Optional[jax.random.KeyArray] = None,\n",
    "  ) -> Tuple[jnp.ndarray, Mapping[str, jnp.ndarray]]:\n",
    "    raise NotImplementedError(\n",
    "        'Autoregressive prediction is not implemented for dual encoder.')\n",
    "\n",
    "  def _encode_batch(self, params: Mapping[str, Array],\n",
    "                    batch: Mapping[str, jnp.ndarray]) -> Array:\n",
    "\n",
    "    return self.module.apply(\n",
    "        {'params': params},\n",
    "        batch['left_encoder_input_tokens'],\n",
    "\n",
    "        enable_dropout=False,\n",
    "        method=self.module.encode)\n",
    "\n",
    "  def _similarity_batch(self,\n",
    "                        params: Mapping[str, Array],\n",
    "                        batch: Mapping[str, jnp.ndarray],\n",
    "                        return_intermediates: bool = False) -> Array:\n",
    "\n",
    "    _, _, logits = self.module.apply({'params': params},\n",
    "                                     batch['left_encoder_input_tokens'],\n",
    "                                     batch['right_encoder_input_tokens'],\n",
    "                                     enable_dropout=False)\n",
    "    return logits\n",
    "\n",
    "  def score_batch(self,\n",
    "                  params: Mapping[str, Array],\n",
    "                  batch: Mapping[str, jnp.ndarray],\n",
    "                  return_intermediates: bool = False) -> jnp.ndarray:\n",
    "\n",
    "    if self._inference_mode not in self.ALLOWED_INFERENCE_MODE:\n",
    "      raise ValueError(\n",
    "          'Invalid `inference_mode`: %s. Supported inference mode: %s.' %\n",
    "          (self._inference_mode, self.ALLOWED_INFERENCE_MODE))\n",
    "    if self._inference_mode == 'encode':\n",
    "      return self._encode_batch(params, batch)\n",
    "    elif self._inference_mode == 'similarity':\n",
    "      return self._similarity_batch(params, batch, return_intermediates)\n",
    "\n",
    "\n",
    "class DualEncoderModel(DualEncoderBase):\n",
    "\n",
    "\n",
    "  ALLOWED_INFERENCE_MODE = frozenset(\n",
    "      {'encode', 'similarity', 'pointwise_similarity'})\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      module: nn.Module,\n",
    "      feature_converter_cls: Callable[[bool], seqio.FeatureConverter],\n",
    "      input_vocabulary: seqio.Vocabulary,\n",
    "      output_vocabulary: seqio.Vocabulary,\n",
    "      optimizer_def: optim.OptimizerDef,\n",
    "      inference_mode: str = 'encode',\n",
    "      use_negatives: bool = False,\n",
    "      use_align_uniform: bool = False,\n",
    "      logit_scale: float = 100,\n",
    "      logit_margin: float = 0.0,\n",
    "  ):\n",
    "\n",
    "\n",
    "\n",
    "    self._use_negatives = use_negatives\n",
    "    self._use_align_uniform = use_align_uniform\n",
    "    self._logit_scale = logit_scale\n",
    "    self._logit_margin = logit_margin\n",
    "    super(DualEncoderModel, self).__init__(\n",
    "        module=module,\n",
    "        feature_converter_cls=feature_converter_cls,\n",
    "        input_vocabulary=input_vocabulary,\n",
    "        output_vocabulary=output_vocabulary,\n",
    "        optimizer_def=optimizer_def,\n",
    "        inference_mode=inference_mode)\n",
    "\n",
    "  def _compute_logits(\n",
    "      self,\n",
    "      params: Mapping[str, Any],\n",
    "      batch: Mapping[str, jnp.ndarray],\n",
    "      dropout_rng: Optional[jnp.ndarray] = None\n",
    "  ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Computes logits via a forward pass of `self.module_cls`.\"\"\"\n",
    "\n",
    "    rngs = {'dropout': dropout_rng} if dropout_rng is not None else None\n",
    "\n",
    "    if not self._use_negatives and 'right_negative_encoder_input_tokens' in batch:\n",
    "      ValueError(\n",
    "          'Invalid module. Please select `DualEncoderWithNegativesModel` for negative inputs.'\n",
    "      )\n",
    "\n",
    "    if self._use_negatives and 'right_negative_encoder_input_tokens' not in batch:\n",
    "      ValueError(\n",
    "          'Invalid inputs. Please prepare negative inputs for DualEncoderWithNegativesModel.'\n",
    "      )\n",
    "\n",
    "    if self._use_negatives:\n",
    "      left_tokens = batch['left_encoder_input_tokens']\n",
    "      right_positive_tokens = batch['right_encoder_input_tokens']\n",
    "      right_negative_tokens = batch['right_negative_encoder_input_tokens']\n",
    "\n",
    " \n",
    "      assert left_tokens.ndim == 2\n",
    "      assert right_positive_tokens.ndim == 2\n",
    "\n",
    "      assert right_negative_tokens.ndim == 2 or right_negative_tokens.ndim == 3\n",
    "\n",
    "      batch_size = right_positive_tokens.shape[0]\n",
    "      assert left_tokens.shape[0] == batch_size\n",
    "      assert right_negative_tokens.shape[0] == batch_size\n",
    "\n",
    "\n",
    "        right_seq_length = right_positive_tokens.shape[1]\n",
    "        assert right_seq_length == right_negative_tokens.shape[2]\n",
    "\n",
    "        num_negatives = right_negative_tokens.shape[1]\n",
    "        right_negative_tokens = jnp.reshape(\n",
    "            right_negative_tokens,\n",
    "            (batch_size * num_negatives, right_seq_length))\n",
    "\n",
    "      (left_encodings, right_encodings,\n",
    "       logits), _ = self.module.apply({'params': params},\n",
    "                                      left_tokens,\n",
    "                                      right_positive_tokens,\n",
    "                                      right_negative_tokens,\n",
    "                                      enable_dropout=rngs is not None,\n",
    "                                      rngs=rngs,\n",
    "                                      mutable='dropout')\n",
    "\n",
    "      left_logits, right_logits = logits, jnp.dot(right_encodings,\n",
    "                                                  left_encodings.transpose())\n",
    "    else:\n",
    "      (left_encodings, right_encodings, logits), _ = self.module.apply(\n",
    "          {'params': params},\n",
    "          batch['left_encoder_input_tokens'],\n",
    "          batch['right_encoder_input_tokens'],\n",
    "          enable_dropout=rngs is not None,\n",
    "          rngs=rngs,\n",
    "          mutable='dropout')\n",
    "\n",
    "      left_logits, right_logits = logits, logits.transpose()\n",
    "\n",
    "    left_logits *= self._logit_scale\n",
    "    right_logits *= self._logit_scale\n",
    "\n",
    "\n",
    "    if dropout_rng is not None:\n",
    "      left_logits = (\n",
    "          left_logits - self._logit_margin *\n",
    "          jnp.eye(N=left_logits.shape[0], M=left_logits.shape[1]))\n",
    "      right_logits = (\n",
    "          right_logits - self._logit_margin * jnp.eye(right_logits.shape[0]))\n",
    "\n",
    "    return left_encodings, right_encodings, left_logits, right_logits\n",
    "\n",
    "  def _compute_loss(\n",
    "      self,\n",
    "      batch: Mapping[str, jnp.ndarray],\n",
    "      left_logits: jnp.ndarray,\n",
    "      right_logits: jnp.ndarray,\n",
    "  ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    # targets: [batch, 1] -> [batch, batch]\n",
    "    left_loss = utils.in_batch_cross_entropy(left_logits)\n",
    "    right_loss = utils.in_batch_cross_entropy(right_logits)\n",
    "    loss = jnp.mean(left_loss + right_loss)\n",
    "    return loss, 0.0, left_logits.shape[0]\n",
    "\n",
    "  def _compute_metrics(\n",
    "      self,\n",
    "      params: Mapping[str, Any],\n",
    "      batch: Mapping[str, jnp.ndarray],\n",
    "      left_logits: jnp.ndarray,\n",
    "      loss: jnp.ndarray,\n",
    "      total_z_loss: jnp.ndarray,\n",
    "      weight_sum: jnp.ndarray,\n",
    "      align_loss: Optional[jnp.ndarray] = None,\n",
    "      uniform_loss: Optional[jnp.ndarray] = None,\n",
    "  ) -> metrics_lib.MetricsMap:\n",
    "    \"\"\"Compute metrics given the logits, targets and loss.\"\"\"\n",
    "    metrics = t5x_models.compute_base_metrics(\n",
    "        logits=left_logits,\n",
    "        targets=utils.sparse_labels_for_in_batch_cross_entropy(left_logits),\n",
    "        mask=None,\n",
    "        loss=loss,\n",
    "        z_loss=total_z_loss)\n",
    "    metrics.update({\n",
    "        'mrr':\n",
    "            metrics_lib.AveragePerStep.from_model_output(\n",
    "                utils.compute_rr(\n",
    "                    left_logits,\n",
    "                    utils.sparse_labels_for_in_batch_cross_entropy(\n",
    "                        (left_logits))))\n",
    "    })\n",
    "    if self._use_align_uniform:\n",
    "      metrics.update({\n",
    "          'align_loss':\n",
    "              metrics_lib.AveragePerStep.from_model_output(align_loss),\n",
    "          'uniform_loss':\n",
    "              metrics_lib.AveragePerStep.from_model_output(uniform_loss),\n",
    "      })\n",
    "    return metrics\n",
    "\n",
    "  def loss_fn(\n",
    "      self,\n",
    "      params: Mapping[str, Any],\n",
    "      batch: Mapping[str, jnp.ndarray],\n",
    "      dropout_rng: Optional[jnp.ndarray],\n",
    "  ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    left_encodings, right_encodings, left_logits, right_logits = self._compute_logits(\n",
    "        params, batch, dropout_rng)\n",
    "\n",
    "    loss, z_loss, weight_sum = self._compute_loss(batch, left_logits,\n",
    "                                                  right_logits)\n",
    "    if self._use_align_uniform:\n",
    "      align_loss = utils.compute_align_loss(left_encodings, right_encodings)\n",
    "      uniform_loss = utils.compute_uniform_loss(\n",
    "          left_encodings) + utils.compute_uniform_loss(right_encodings)\n",
    "      metrics = self._compute_metrics(params, batch, left_logits, loss, z_loss,\n",
    "                                      weight_sum, align_loss, uniform_loss)\n",
    "    else:\n",
    "      metrics = self._compute_metrics(\n",
    "          params,\n",
    "          batch,\n",
    "          left_logits,\n",
    "          loss,\n",
    "          z_loss,\n",
    "          weight_sum,\n",
    "      )\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "  def score_batch(self,\n",
    "                  params: Mapping[str, Array],\n",
    "                  batch: Mapping[str, jnp.ndarray],\n",
    "                  return_intermediates: bool = False) -> jnp.ndarray:\n",
    " \n",
    "    if self._inference_mode not in self.ALLOWED_INFERENCE_MODE:\n",
    "      raise ValueError(\n",
    "          'Invalid `inference_mode`: %s. Supported inference mode: %s.' %\n",
    "          (self._inference_mode, self.ALLOWED_INFERENCE_MODE))\n",
    "    if self._inference_mode == 'encode':\n",
    "      return self._encode_batch(params, batch)\n",
    "    elif self._inference_mode == 'similarity':\n",
    "      return self._similarity_batch(params, batch, return_intermediates)\n",
    "    elif self._inference_mode == 'pointwise_similarity':\n",
    "      logits = self._similarity_batch(params, batch, return_intermediates)\n",
    "      return jnp.diagonal(logits)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
